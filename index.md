<div style="display:flex; align-items:flex-start; gap:16px;">
  <img src="IMG_7217.JPG" alt="Alt text" style="width:180px; max-width:40vw; height:auto; border-radius:6px;">
  <div markdown="1">
    My name is Neelima Prasad and I am a third year Ph.D. candidate in Computer Science at the University of Colorado Boulder, advised by Professor Danna Gurari in the [Image and Video Computing (IVC) Group](https://home.cs.colorado.edu/~DrG/IVC_Group.html). My research centers on developing vision systems that can accurately segment and track objects and their constituent parts across images and videos. I focus on hierarchical instance segmentation, multi-object tracking, and the creation of fine-grained datasets that enable richer reasoning about object structure. Broadly, I am motivated by building models that understand not only what an object is, but how its parts relate and evolve over time, with applications in accessibility, assistive technologies, and privacy-aware video understanding.
    Before beginning my Ph.D., I worked as a Computer Scientist at the Naval Air Systems Command (NAVAIR), where I developed reinforcement learning systems for various defense simulations and designed tools for radar and electronic warfare analysis. Earlier, I spent several years as a research intern at Los Alamos National Laboratory, contributing to projects in cryptography, network analysis, and astrophysics, including co-authoring a publication on gamma-ray burst modeling. I earned my B.S. in Mathematics & Computer Science from UC San Diego, and have since gained experience across industry and research labs. Currently I am employed as an AI engineering intern at LightTable where I hope to  to continue to bridge academic innovation and industry-scale deployment in computer vison. I am excited to continue shaping research that pushes the field forward while opening new possibilities for how intelligent visual systems can support real-world applications.
    
  </div>
</div>



[CV](CV_Everley_Tseng_Sep25.pdf) • [LinkedIn](https://www.linkedin.com/in/everley-tseng/) • [Google Scholar](https://scholar.google.com/citations?user=Ul6YnP8AAAAJ) • [GitHub](https://github.com/EverleyTseng) • [Email](mailto:yu-yun.tseng@colorado.edu)


## EDUCATION

**PhD**, Computer Science, University of Colorado Boulder *(2021-Present)*

**MS**, Computational Intelligence, College of AI, National Chiao Tung University *(2019-2021)*

**BS**, Engineering and System Science, National Tsing Hua University *(2013-2017)*

## WORK EXPERIENCES

**AI/ML Intern**, Marvell Semiconductor, Inc. *(2025-2025)*

**Computer Vision Engineer**, Coretronic Corporation *(2017-2019)*

**Research Intern**, Lite-On Technology Corporation *(2017-2017)*

## PUBLICATIONS

**Visual Privacy Management with Generative AI for Blind and Low-Vision People**, Tanusree Sharma, Yu-Yun Tseng, Lotus Zhang, Ayae Ide, Kelly Avery Mack, Leah Findlater, Danna Gurari, Yang Wang. *The 26th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS)*, 2025. \[[paper](https://arxiv.org/abs/2507.00286)\]

**Acknowledging Focus Ambiguity in Visual Questions**, Chongyan Chen, Yu-Yun Tseng, Zhuoheng Li, Anush Venkatesh, Danna Gurari. *International Conference on Computer Vision (ICCV)*, 2025. \[[paper](https://arxiv.org/abs/2507.00286](https://arxiv.org/abs/2501.02201))\]

**BIV-Priv-Seg: Locating Private Content in Images Taken by People With Visual Impairments**, Yu-Yun Tseng, Tanusree Sharma, Lotus Zhang, Abigale Stangl, Leah Findlater, Yang Wang, Danna Gurari. Oral presentation (top 8%), *IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)*, 2025. \[[paper](https://openaccess.thecvf.com/content/WACV2025/papers/Tseng_BIV-Priv-Seg_Locating_Private_Content_in_Images_Taken_by_People_with_WACV_2025_paper.pdf), [oral presentation](https://www.youtube.com/watch?v=78WliClpPJI)\]

**Learning-Based WiFi Fingerprint Inpainting via Generative Adversarial Networks**, Yu Chan, Pin-Yu Lin, Yu-Yun Tseng, Jen-Jee Chen, Yu-Chee Tseng.
*IEEE International Conference on Computer Communications and Networks (ICCCN)*, 2024. \[[paper](https://ieeexplore.ieee.org/document/10637537)\]

**Designing Accessible Obfuscation Support for Blind Individuals’ Visual Privacy Management**, Lotus Zhang, Tanusree Sharma, Abigale Stangl, Yu-Yun Tseng, Inan Xu, Danna Gurari, Yang Wang, Leah Findlater. *ACM Conference on Human Factors in Computing Systems (CHI)*, 2024. \[[paper](https://dl.acm.org/doi/10.1145/3613904.3642713)\]

**Disability-First Design and Creation of A Dataset with Private Visual Information**, Tanusree Sharma, Abigale Stangl, Lotus Zhang, Yu-Yun Tseng, Inan Xu, Leah Findlater, Danna Gurari, Yang Wang. *ACM Conference on Human Factors in Computing Systems (CHI)*, 2023. \[[paper](https://dl.acm.org/doi/10.1145/3544548.3580922)\]

**VizWiz-FewShot: Locating Objects in Images Taken by People With Visual Impairments**, Yu-Yun Tseng, Alexander Bell, Danna Gurari. *European Conference on Computer Vision (ECCV)*, 2022. \[[paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680563.pdf)\]

**Efficient Vehicle Counting Based on Time-Spatial Images by Neural Networks**, Yu-Yun Tseng, Tzu-Chien Hsu, Jen-Jee Chen, Yu-Chee Tseng. *IEEE International Conference on Mobile Ad-Hoc and Smart Systems (MASS)*, 2021. \[[paper](https://ieeexplore.ieee.org/document/9637752)\]

**Computer Vision-Assisted Instant Alerts in 5G**, Yu-Yun Tseng, Po-Min Hsu, Jen-Jee Chen, Yu-Chee Tseng. *IEEE International Conference on Computer Communications and Networks (ICCCN)*, 2020. \[[paper](https://ieeexplore.ieee.org/document/9209751)\]

## Talks

- Area Exam - University of Colorado Boulder \[[video](https://www.youtube.com/watch?v=jat4_sOnd6U)\]
- Vision and Color Summer Data Blast - OPTICA \[[page](https://www.optica.org/get_involved/technical_groups/vc/vision_and_color_summer_data_blast/)\]

